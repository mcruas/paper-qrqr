\section{Quantile Regression based time series model} \label{sec:qr1}

Let the $\alpha$-conditional quantile function of $Y$ for a given value $x$ of the $d$-dimensional random variable $X$, i.e., $Q_{Y|X}:[0,1] \times \mathbb{R}^d \rightarrow \mathbb{R}$, can be defined as %(in short, from now on, $Q_{Y|X}(\cdot, \cdot)$)
\begin{equation}
Q_{Y|X}(\alpha,x) = F_{Y|X}^{-1}(\alpha,x) = \inf\{y: F_{Y|X}(y,x) \geq \alpha\}.
\label{eq:quantile-function}
\end{equation}
Let a dataset be composed from $\{y_t,x_t \}_{t \in T}$, where $T$ is the set of time indexes, let $\rho$ be the check function 
\begin{equation}\label{eq:check-function}
\rho_{\alpha}(x)=\begin{cases}
\alpha x & \text{if }x\geq0\\
(1-\alpha)x & \text{if }x<0
\end{cases}.
\end{equation}
The sample quantile function for a given probability $\alpha$ is based on a finite number of observations and is the solution to minimizing the loss function $L_\alpha(\cdot)$:
\begin{IEEEeqnarray}{C}
\hat{Q}_{Y|X}(\alpha,\cdot)\quad\in\quad  \underset{q\in\mathcal{Q}}{\text{arg min}}\, L_\alpha(q) = \sum_{t\in T}\rho_{\alpha}(y_{t}-q(x_t)),\label{eq:optim-lqr1} 
\end{IEEEeqnarray}
The $\alpha$-quantile function $q(\cdot)$ belongs to a function space $\mathcal{Q}$. We might have different assumptions for space $\mathcal{Q}$, depending on the type of function we want to find for $q$. A few properties, however, must be achieved by our choice of space, such as being continuous and having limited first derivative. In this paper, we consider the case where $\mathcal{Q}$ is a linear function's space, so we have that 
$$q_\alpha(x_t) = \beta_0 + \beta^T x_t.$$

The quantile function is approximated by a sequence of $|J|$ (where $J$ is an index set) quantiles $q_{\alpha_1} \leq q_{\alpha_2} \leq \dots \leq q_{\alpha_{|J|}}$. 
Denote the closely related set $A = \{ \alpha_j \mid j \in J \}$, whose elements lies on $[0,1]$, such that $0 < \alpha_1 < \alpha_2 < \dots < \alpha_{|J|} < 1$. %The sequence $\{ \alpha_j \}_{j \in J}$ provides a finite discretization of the interval $[0,1]$. 

We apply the QR model to estimate the conditional distribution, $\hat{Q}_{Y_{t+h}|X_{t+h},Y_t, Y_{t-1}, \dots} (\alpha,\cdot)$, of time series $\{y_t\}_{t \in T}$ $h$-steps ahead, where $X_{t+h}$ makes reference to a vector of exogenous variables. Once the conditional distribution is estimated, future scenarios can be obtained by simulation. %In this paper, we focus on the 1-step ahead forecasting ($h=1$) and the model don't include any covariates but its own autoregressive terms. 

\subsection*{Regularization on selection of covariates and coefficient slope}
When dealing with many candidates of covariates, one has to deal with the problem of selecting the relevant ones out of a set of candidates which improve model fit. In practice, it means that some coefficients from the vector $\beta_j = [ \beta_{1 j} \cdots \beta_{pj} ]$ might assume zero value.
There are many ways of selecting a subset of variables among the available options.
A classical approach for this problem is the Stepwise algorithm \cite{efroymson1960multiple}, \cite{hocking_selection_1967}, \cite{tibshirani1996regression}, which includes variables in sequence. 

A popular way of variable selection which fits on a linear programming context are the LASSO/AdaLASSO techniques, which consists in penalize the $\ell_1$-norm. Besides shrinking coefficients towards to zero, it has also the capability of effectively pushing coefficients to zero (an effect that ridge regression cannot achieve \cite{tibshirani1996regression}). The usage of LASSO and AdaLASSO in QR context is the topic of study in \cite{li_l1-norm_2008,ciuperca_adaptive_2016,belloni_l1-penalized_2009,zou_regularized_2008,jiang_interquantile_2014}.
We refer to the reader the work from \cite{belloni_l1-penalized_2009}, where it is possible to find specific properties and convergence rates when using the LASSO to perform model selection in a quantile regression framework. 

Regarding the penalization parameter $\lambda$, which dictates the shrinkage magnitude of the linear coefficients, the level of parsimony of the model can be defined by the user through such quantity. This is due to the fact that higher values of $\lambda$ means less variables selected to be non-zero. 

The single $\alpha$-quantile AdaLASSO is estimated by the following optimization problem:
\begin{IEEEeqnarray}{c}
\underset{\beta_{0},\beta}{\text{min}} \sum_{t \in T} \rho_\alpha(y_t - (\beta_0 + \sum_{p \in P} \beta_p x_{tp})) +\lambda \sum_{p \in P} w_p | \beta_p |.\label{eq:l1-qar-optim} 
\end{IEEEeqnarray}
What differs the AdaLASSO from the LASSO is the inclusion of the term $w_p$. Suppose the model (\ref{eq:l1-qar-optim}) is estimated with all $w_{pj}=1$, the output of the  optimization problem are coefficients $\beta^{LASSO}_{pj}$. The AdaLASSO coefficients $\beta^{AdaLASSO}_{pj}$ are obtained when solving the same optimization problem while letting $w_{pj}=1/|\beta^{LASSO}_{pj}|$. 

As we are interested on the conditional distribution as a whole, we estimate multiple quantiles at once. In order to produce a coherent distribution function, the output of the problem must respect certain properties, such as being monotone increasing. 
Besides that, one can expect that the value of similar quantiles be produced by similar models. If the coefficient of a given $p$ covariate changes too abruptly with respect to a change on the probability $\alpha$, there is a high probability that the estimation did not produce good results rather than this noise belonging to the true model.  In order to correct this much common behavior in QR estimation, we introduce a second derivative filter, given by the discrete approximation shown below:
\begin{equation}
D_{pj}^{2}:=\frac{\left(\frac{\beta_{p,j+1}-\beta_{pj}}{\alpha_{j+1}-\alpha_{j}}\right)-\left(\frac{\beta_{p,j}-\beta_{p,j-1}}{\alpha_{j}-\alpha_{j-1}}\right)}{\alpha_{j+1}-\alpha_{j-1}}.
\end{equation}
With this approach, one can keep track on the crossing quantiles issue as well as using a interquartile structure as a strategy to reduce noise on estimation %! explicar melhor o porque
The works by \cite{zou_regularized_2008, jiang_interquantile_2014} also use multiple quantile regressions at once and make use of interquantile similarities to produce regularization on the quantiles. In \cite{zou_regularized_2008}, the author uses the norm $\| \beta \|_{1\infty}=\sum_{p=1}^{|P|} \max\{ |\beta_j^{(k)} |\}$ as penalization. Such penalization is imposed on the maximum value among all quantiles for a given covariate. This idea is extended by \cite{jiang_interquantile_2014}, that uses a fused AdaLASSO mixing the LASSO penalization with the absolute interquantile difference.

\subsection{Quantile Regularized Adaptive LASSO (QRAL)}

The statistical model defined by the model QRAL is defined by the vector of coefficients $\beta_{0}$ and the matrix of size $|P| \times |J|$ of regressor coefficients $\beta_{pj}$. These coefficients are the solution from the minimization problem given below:
	\begin{IEEEeqnarray}{lr}
	\underset{\beta_{0j},\beta_j}{\text{min}} \sum_{j \in J} \left( \sum_{t\in T}\rho_{\alpha_j}(y_{t}-(\beta_{0j} + \beta_j^T x_t)) \right.  \span \nonumber \\
	\span \left. + \lambda\  \sum_{p \in P} w_{pj}^\delta \mid  \beta_{pj} \mid \right) + \gamma \sum_{p \in P} \sum_{j \in J'} |D^2_{pj}|,\label{eq:adalasso_model}
\end{IEEEeqnarray}
where the weights $w_{pj} = 1/\tilde{\beta}_{pj}$ and $\tilde \beta_{pj}$ are the coefficients from the first-step LASSO estimation. The parameter $\delta$ is an exponential parameter usually set to 1.
The sum of absolute values that compose the second derivative filter $\sum_{j \in J'}\sum_{p \in P}|D_{pj}^{2}|$ is added on the objective function multiplied by a tuning parameter $\gamma$, where the set $J'=\{2,\dots,|J|-1 \}$.
