\section{Quantile Regression based time series model} \label{sec:qr1}

Let the $\alpha$-conditional quantile function of $Y$ for a given value $x$ of the $d$-dimensional random variable $X$, i.e., $Q_{Y|X}:[0,1] \times \mathbb{R}^d \rightarrow \mathbb{R}$, can be defined as %(in short, from now on, $Q_{Y|X}(\cdot, \cdot)$)
	\begin{equation}
	Q_{Y|X}(\alpha,x) = F_{Y|X=x}^{-1}(\alpha) = \inf\{y: F_{Y|X=x}(y) \geq \alpha\}.
	\label{eq:quantile-function}
	\end{equation}
The Conditional Quantile Function $Q$ is the inverse of the Distribution Function $F$, and represents the smallest value $y$ for which the distribution function is greater than a given probability $\alpha$.

Let $\rho$ be the check function 
	\begin{equation}\label{eq:check-function}
	\rho_{\alpha}(x)=\begin{cases}
	\alpha x & \text{if }x\geq0\\
	(1-\alpha)x & \text{if }x<0
	\end{cases}.
	\end{equation}
The quantile function for a given finite sample $\{y_t,x_t \}_{t \in T}$, where $T$  is the set of time indexes, and a given probability $\alpha$ is the solution of minimizing the loss function $L_\alpha(\cdot)$:
	\begin{IEEEeqnarray}{C}
	\hat{Q}_{Y|X}(\alpha,\cdot)\quad\in\quad  \underset{q\in\mathcal{Q}}{\text{arg min}}\, L_\alpha(q) = \sum_{t\in T}\rho_{\alpha}(y_{t}-q(x_t)).\label{eq:optim-lqr1} 
	\end{IEEEeqnarray}
For Inference on Quantile Regression and finite sample properties, see the Chapter 3 on \cite{koenker2005quantile}.
The $\alpha$-quantile function $q(\cdot)$ belongs to a function space $\mathcal{Q}$. We might have different assumptions for space $\mathcal{Q}$, depending on the type of function we want to find 
for $q$. A few properties, however, must be achieved by our choice of space, such as being continuous and having limited first derivative. In this work, only the case where $\mathcal{Q}$ is a linear function space is considered. 
So, $q$ is of the form $$q_\alpha(x_t) = \beta_0 + \beta^T x_t.$$ 

% The linear model presumes that the $\alpha$-quantile function is a linear function of its regressors:
% $$q_{\alpha}(x_t) = \beta_{0} + \beta^T x_t.$$   
When dealing with many candidates of covariates, one has to deal with the problem of selecting the relevant ones out of a set of candidates which improve model fit. In practice, it means that some coefficients from the vector $\beta = [ \beta_{1 } \cdots \beta_{p} ]$ might assume zero value.
There are many ways of selecting a subset of variables among the available options.
A classical approach for this problem is the Stepwise algorithm \cite{efroymson1960multiple}, \cite{hocking_selection_1967}, \cite{tibshirani1996regression}, which includes variables in sequence. 

A popular way of variable selection which fits on a linear programming context are the LASSO/AdaLASSO techniques, which consists in penalizing the $\ell_1$-norm. Besides shrinking coefficients towards to zero, it has also the capability of effectively pushing coefficients to zero (an effect that ridge regression cannot achieve \cite{tibshirani1996regression}). The usage of LASSO and AdaLASSO in QR context is the topic of study in \cite{li_l1-norm_2008,ciuperca_adaptive_2016,belloni_l1-penalized_2009,zou_regularized_2008,jiang_interquantile_2014}.
We refer to the reader the work from \cite{belloni_l1-penalized_2009}, where it is possible to find specific properties and convergence rates when using the LASSO to perform model selection in a quantile regression framework. 

Regarding the penalization parameter $\lambda$, which dictates the shrinkage magnitude of the linear coefficients, the level of parsimony of the model can be defined by the user through such quantity. This is due to the fact that higher values of $\lambda$ means less variables selected to be non-zero. 

The single $\alpha$-quantile AdaLASSO is estimated by the following optimization problem:
\begin{IEEEeqnarray}{c}
\underset{\beta_{0},\beta}{\text{min}} \sum_{t \in T} \rho_\alpha(y_t - (\beta_0 + \sum_{p \in P} \beta_p x_{tp})) +\lambda \sum_{p \in P} w_p | \beta_p |.\label{eq:l1-qar-optim} 
\end{IEEEeqnarray}
What differs the AdaLASSO from the LASSO is the inclusion of the term $w_p$. Suppose the model (\ref{eq:l1-qar-optim}) is estimated with all $w_{p}=1$, the output of the optimization problem are coefficients of LASSO  $\beta^{L}_{p}$. The AdaLASSO coefficients $\beta^{AL}_{p}$ are obtained when solving the same optimization problem while letting $w_{p}=1/|\beta^{L}_{p}|$. 
% What differs the AdaLASSO from the LASSO is the inclusion of the term $w_p$. Suppose the model (\ref{eq:l1-qar-optim}) is estimated with all $w_{pj}=1$, the output of the optimization problem are coefficients of LASSO  $\beta^{L}_{pj}$. The AdaLASSO coefficients $\beta^{AL}_{pj}$ are obtained when solving the same optimization problem while letting $w_{pj}=1/|\beta^{L}_{pj}|$. % para voltar o 'j'




