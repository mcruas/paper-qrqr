\section{conditional distribution based on Quantile Regression for Time Series}

In the previous section, we presented a linear model for estimating a single $\alpha$-quantile using QR with adaLASSO as a regularization strategy to select the best covariates. However, to build a CDF from an array of quantiles, we propose jointly estimating them in order to explore the connection across different quantiles. This will be discussed in the sequel.

Let the finite discretization of the interval $[0,1]$ be composed of a sequence of probabilities $0 < \alpha_1 < \alpha_2 < \dots < \alpha_{|J|} < 1$ and denote $A$ as the set $A = \{ \alpha_j \mid j \in J \}$, where $J$ is an index set for the probabilities $\alpha$. 
The $\alpha$-quantiles are, from this point forward, indexed by $j$, to account for the different models that are simultaneously estimated. A property that must be respected is the monotonicity of the quantile function $Q$, such that $q_{\alpha_1} \leq q_{\alpha_2} \dots \leq q_{\alpha_{|J|}}$.
The sequence of quantiles defines a continuous quantile function after interpolation, from which a CDF can be obtained after inverting the estimated quantile function.

% todo rever essa parte , daqui até o fim da seção

To produce a proper distribution function via the estimation of several quantile functions, the output of the problem must respect certain properties, such as being monotonically increasing. 
If a sequence of quantiles do not respect such a property, the issue is known as crossing quantiles.
In addition to monotonicity, one can expect that the value of similar quantiles can be produced by similar models. 
If the coefficient of a given $p$ covariate changes too abruptly with respect to a change in the probability $\alpha$, there is a high probability that the estimation did not produce good results and captured noise in the data.  
To account for all these issues, all quantiles must be estimated at once. 

To ensure smooth transitions on the estimation of coefficients across quantiles, we introduce a $\ell_1$-norm second derivative filter (more of this topic in \cite{kim2009ell_1}). Assuming the function $\beta(\alpha)$ maps the quantile probability $\alpha$ on its coefficient value, the discrete approximation of the second derivative of $\beta(\alpha)$ is given by:
\begin{equation}
D_{\alpha_j}^{2} \beta_{pj} := \frac{\left(\frac{\beta_{p,j+1}-\beta_{pj}}{\alpha_{j+1}-\alpha_{j}}\right)-\left(\frac{\beta_{p,j}-\beta_{p,j-1}}{\alpha_{j}-\alpha_{j-1}}\right)}{\alpha_{j+1}-\alpha_{j-1}}. 
\end{equation}
In \cite{zou_regularized_2008, jiang_interquantile_2014}, multiple quantile regressions are also used at once and make use of interquantile similarities to produce regularization on the quantiles. In \cite{zou_regularized_2008}, the authors use the norm $\| \beta \|_{1\infty}=\sum_{p=1}^{|P|} \max\{ |\beta_j^{(k)} |\}$ as penalization. Such penalization is imposed on the maximum value among all quantiles for a given covariate. This idea is extended by \cite{jiang_interquantile_2014}, which uses a fused adaLASSO, mixing the $\ell_1$ penalization with the absolute interquantile difference (first derivative).

The statistical model we propose, named \textbf{quantile regularized adaptive LASSO (QRAL)}, is defined by the vector of coefficients $\beta_{0j}$ and the matrix of size $|P| \times |J|$ of regressor coefficients $\beta_{pj}$, which are the solution to the following minimization problem:
% \begin{IEEEeqnarray}{lr}  % para duas colunas
% 	\underset{\beta_{0j},\beta_j}{\text{min}} \sum_{j \in J} \left( \sum_{t\in T}\rho_{\alpha_j}(y_{t}-(\beta_{0j} + \beta_j^T x_t)) \right.  \span \nonumber \\
% 	\span \left. + \lambda\  \sum_{p \in P} w_{pj}^\delta \mid  \beta_{pj} \mid \right) + \gamma \sum_{p \in P} \sum_{j \in J'} |D^2_{pj}|,\label{eq:adaLASSO_model}
% \end{IEEEeqnarray}
\begin{IEEEeqnarray}{lr} % para duas colunas
  \underset{\beta_{0j},\beta_j}{\text{min}} \sum_{j \in J} \left( \sum_{t\in T}\rho_{\alpha_j}(y_{t}-(\beta_{0j} + \beta_j^T x_t)) \right. \span \nonumber \\  
  \span \left. + \lambda\    \sum_{p \in P} w_{pj}^\delta \mid  \beta_{pj} \mid \right) + \gamma \sum_{p \in P} \sum_{j \in J'} |D^2_{\alpha_j}\beta_{pj}|, \label{eq:adaLASSO_model_mat1}\\
  \text{subject to:} \span \nonumber \\
	\beta_{0j} + \beta_{j}^T x_{t} \leq \beta_{0,j+1} + \beta_{j+1}^T x_{t},& \forall t \in T, \forall j \in J_{(-1)}, \label{eq:adaLASSO_model_mat2} 
\end{IEEEeqnarray}
where $\tilde \beta_{pj}$ are the coefficients from the first-step LASSO estimation and the weights $w_{pj} = 1/\tilde{\beta}_{pj}$. The parameter $\delta$ is an exponential parameter and is usually set to 1.
The sum of the absolute values that compose the second derivative filter $\sum_{j \in J'}\sum_{p \in P}|D_{\alpha_j}^{2}\beta_{pj}|$ is added on the objective function multiplied by a tuning parameter $\gamma$. Finally, $J_{(-1)} = \{ 2, \dots, |J| \}$ is the set containing all indexes but the first and $J' = \{ 2, \dots, |J|-1 \}$ is the set containing all indexes but the first and the last. These two auxiliary sets are used to implement the constraint (\ref{eq:adaLASSO_model_mat2}), which ensures a monotonic quantile function by forcing that, for every $x_t$ and $\alpha_j$--quantile, $q_{\alpha_{j}}(x_t) \leq q_{\alpha_{j+1}}(x_t)$.

With this approach, one can keep track of the aforementioned crossing quantiles issue  
as well as using an inter-quartile coherence structure as a strategy to produce a single model where all quantiles are, to some extent, connected to each other. The connection between quantiles is given by the last penalization term of expression \eqref{eq:adaLASSO_model_mat1}, which ensures a piecewise-linear (continuous) format for the variation of the coefficients across quantiles \cite{kim2009ell_1}. % todo  explicar melhor o porquê 
