\section{conditional distribution based on Quantile Regression for Time Series}

In the previous section, we presented a linear model for estimating a single $\alpha$-quantile by using QR with adaLASSO as a regularization strategy to select the best model. However, to build a CDF from an array of quantiles, we propose to jointly estimate them, in order to explore the connection across different quantiles. 

Let the finite discretization of the interval $[0,1]$ be composed of a sequence of probabilities $0 < \alpha_1 < \alpha_2 < \dots < \alpha_{|J|} < 1$ and denote as $A$ the set $A = \{ \alpha_j \mid j \in J \}$, where $J$ is an index set for the probabilities $\alpha$. 
The $\alpha$-quantiles are, from this point forward, indexed by $j$, to account for the different models that are simultaneously estimated. A property that must be respected is the monotonicity of the quantile function $Q$, such that $q_{\alpha_1} \leq q_{\alpha_2} \dots \leq q_{\alpha_{|J|}}$.
The sequence of quantiles define a continuous quantile function after interpolation, and finally a CDF after inverting the estimated quantile function.




As we are interested on the conditional distribution as a whole, we estimate multiple quantiles at once. In order to produce a distribution function, the output of the problem must respect certain properties, such as being monotonically increasing. 
Besides that, one can expect that the value of similar quantiles be produced by similar models. If the coefficient of a given $p$ covariate changes too abruptly with respect to a change on the probability $\alpha$, there is a high probability that the estimation did not produce good results, capturing noise on the data.  In order to correct this much common behavior in QR estimation, we introduce a second derivative filter,given by the discrete approximation shown below:
\begin{equation}
D_{\alpha_j}^{2} \beta_{pj} := \frac{\left(\frac{\beta_{p,j+1}-\beta_{pj}}{\alpha_{j+1}-\alpha_{j}}\right)-\left(\frac{\beta_{p,j}-\beta_{p,j-1}}{\alpha_{j}-\alpha_{j-1}}\right)}{\alpha_{j+1}-\alpha_{j-1}}. 
\end{equation}
With this approach, one can keep track of the crossing quantiles issue \footnote{Definição de Crossing Quantile} % todo crossing quantile
as well as using a interquartile structure as a strategy to reduce noise on estimation % todo  explicar melhor o porquê 
The works by \cite{zou_regularized_2008, jiang_interquantile_2014} also use multiple quantile regressions at once and make use of interquantile similarities to produce regularization on the quantiles. In \cite{zou_regularized_2008}, the author uses the norm $\| \beta \|_{1\infty}=\sum_{p=1}^{|P|} \max\{ |\beta_j^{(k)} |\}$ as penalization. Such penalization is imposed on the maximum value among all quantiles for a given covariate. This idea is extended by \cite{jiang_interquantile_2014}, that uses a fused AdaLASSO mixing the LASSO penalization with the absolute interquantile difference.

The statistical model \textbf{Quantile Regularized Adaptive LASSO (QRAL)} is defined by the vector of coefficients $\beta_{0}$ and the matrix of size $|P| \times |J|$ of regressor coefficients $\beta_{pj}$. These coefficients are the solution of the minimization problem given below:
% \begin{IEEEeqnarray}{lr}  % para duas colunas
% 	\underset{\beta_{0j},\beta_j}{\text{min}} \sum_{j \in J} \left( \sum_{t\in T}\rho_{\alpha_j}(y_{t}-(\beta_{0j} + \beta_j^T x_t)) \right.  \span \nonumber \\
% 	\span \left. + \lambda\  \sum_{p \in P} w_{pj}^\delta \mid  \beta_{pj} \mid \right) + \gamma \sum_{p \in P} \sum_{j \in J'} |D^2_{pj}|,\label{eq:adalasso_model}
% \end{IEEEeqnarray}
\begin{IEEEeqnarray}{lr} % para uma coluna
  \underset{\beta_{0j},\beta_j}{\text{min}} \sum_{j \in J} \left( \sum_{t\in T}\rho_{\alpha_j}(y_{t}-(\beta_{0j} + \beta_j^T x_t)) \right. \span \\  
  \span \left. + \lambda\    \sum_{p \in P} w_{pj}^\delta \mid  \beta_{pj} \mid \right) + \gamma \sum_{p \in P} \sum_{j \in J'} |D^2_{\alpha_j}\beta_{pj}|, \label{eq:adalasso_model_mat1}\\
  \text{subject to} \span \nonumber \\
	\beta_{0j} + \beta_{j}^T x_{t} \leq \beta_{0,j+1} + \beta_{j+1}^T x_{t},& \forall t \in T, \forall j \in J_{(-1)}, \label{eq:adalasso_model_mat2} 
\end{IEEEeqnarray}
where the weights $w_{pj} = 1/\tilde{\beta}_{pj}$ and $\tilde \beta_{pj}$ are the coefficients from the first-step LASSO estimation. The parameter $\delta$ is an exponential parameter usually set to 1.
The sum of absolute values that compose the second derivative filter $\sum_{j \in J'}\sum_{p \in P}|D_{\alpha_j}^{2}\beta_{pj}|$ is added on the objective function multiplied by a tuning parameter $\gamma$, where the set $J'=\{2,\dots,|J|-1 \}$.
