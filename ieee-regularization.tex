\section{Regularization on the covariates} \label{sec:regularization}


Two ways of selecting variables will be employed. In the first we use a Mixed Integer Linear Programming optimization problem (MILP) to find the best subset among all possible subsets of covariates. 




\subsection{Variable selection via LASSO}
\label{sec:best-subset-ell1}

In our problem, however, we need to estimate multiple quantiles at once, in order to being able to circumvent the crossing quantiles issue. 

The process of estimation is done in two stages: (i) variable selection and (ii) coefficients estimation. At first, all normalized covariates\footnote{For such estimation to be coherent each covariate must have the same relative weight in comparison with one another, i.e., they must be normalized. 
This normalization process is a linear transformation to each covariate such that all have mean $\mu = 0$ and variance $\sigma^2 = 1$. 
We apply the transformation $\tilde{x}_{t,p} = (x_{t,p} - \bar{x}_{t,p}) / \hat\sigma_{x_{t,p}}$, where $\bar{x}_{t,p}$ and $\hat{\sigma}_{x_{t,p}}$ are respectively the sample's unconditional mean and standard deviation. The $\tilde{y}_{t-p,i}$ series will be used to estimate the coefficients, as this series has the desired properties.} are input on the following optimization problem:
\begin{IEEEeqnarray}{lr}
\tilde \beta_\lambda^{*} = \underset{\beta_{0j},\beta_j,\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{arg min}} \sum_{j \in J} \left( \sum_{t \in T}(\alpha_j\varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{t j}^{-}) \right. \span \nonumber \\
\span \left. +\lambda\sum_{p \in P} (\xi^+_{pj} + \xi^-_{pj}) \right),   \label{eq:obj-lasso} \\
\mbox{subject to: } \nonumber & \\
\varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}=y_{t}-\beta_{0 j}-\beta_{j}^T x_{t},& \forall t \in T ,\forall j \in J,\\
\xi^+_{pj} - \xi^-_{pj} = \beta_{p j},&\forall p\in P, \forall j \in J,  \label{l1-qar-3}
\\
\beta_{0j} + \beta_{j}^T x_{t} \leq \beta_{0,j+1} + \beta_{j+1}^T x_{t},&\forall t \in T, \forall j \in J_{(-1)}, \\
\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}, \xi_{t j}^{+},\xi_{t j}^{-} \geq0,&\forall t \in T, \forall j \in J,\label{eq:obj-lasso-end}
\end{IEEEeqnarray}
This model is built upon the standard linear programming model for the quantile regression (\ref{eq:linear-opt-1})-(\ref{eq:linear-opt-ult}). 
On the above formulation, the $\ell_1$-norm of equation (\ref{eq:l1-qar-optim}) is substituted by the sum $\xi^+_{pj} + \xi^-_{pj}$, for each $j$, which represents the absolute value of $\beta_{pj}$. 

% % % % % % % % Esse trecho deve ser usado quando os coeficientes do lasso forem utilizados, e não apenas o lasso sendo um seletor de variáveis % % % % % % % % %
%Each component of the output $\tilde \beta_\lambda^{*LASSO}$ must be corrected by multiplying each coefficient for its standard deviation: $\beta_{p\alpha,\lambda}^{*LASSO} = \tilde \beta_{p\alpha,\lambda}^{*LASSO} \hat\sigma_{x_{t,p}}$.
%\begin{eqnarray}
%\tilde \beta_\lambda^{*LASSO} = \underset{\beta_{0},\beta,\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{arg min}} & \sum_{j \in J} \sum_{t \in T}\left(\alpha\varepsilon_{t j}^{+}+(1-\alpha)\varepsilon_{t j}^{-}\right)+\lambda\sum_{p \in P}\mbox{\ensuremath{\xi}}_{p \alpha} \label{eq:obj-lasso} \\
%\mbox{subject to } & \varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}= y_{t}-\beta_{0 \alpha}-\sum_{p \in P}\beta_{p j}\tilde x_{t,p},&\forall t\in T, \forall j \in J, \\
%& \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}\geq0,&\forall t \in T, \forall j \in J,\\
%& \xi_{pj}\geq\beta_{p j},&\forall p\in P, \forall j \in J,  \label{l1-qar-3}
%\\
%& \xi_{pj}\geq-\beta_{p j},&\forall p\in P, \forall j \in J.  \label{l1-qar-4}
%\end{eqnarray}

For low values of $\lambda$, the penalty over the size of coefficients is small, making the output of problem (\ref{eq:obj-lasso})-(\ref{eq:obj-lasso-end}) be composed mainly of nonzero coefficients. On the other hand, when the penalty on $\| \beta_j \|_1$ is big, many covariates will have zero valued coefficients. When $\lambda$ approaches infinity, one has a constant model. 
Note the linear coefficient $\beta_{0j}$ is not penalized.

As the LASSO coefficients are shrunk towards zero they become biased. Our strategy will be to employ the LASSO as a variable selector, and estimate coefficients with regular QR on a second stage. 
The optimum vector of coefficients $\tilde \beta_\lambda^{*}$ on the first stage may be composed by both nonzero and zero coefficients, for a given $\lambda$ . 
We then define $S_\lambda$ as the set of indexes of selected variables given by
\begin{equation*}
S_\lambda = \{ p \in \{ 1,\dots,P \} | \; |\tilde \beta^{*}_{\lambda,p}| \neq 0  \}.
\end{equation*}
Hence, we have that, for each $p \in \{ 1,\dots,P \}$,
$$\beta^{*LASSO}_{\lambda,p} = 0 \Longrightarrow \beta^{*}_{\lambda,p} = 0.$$

On the second stage, the optimal coefficient vector $\tilde \beta_\lambda^{*}$ is estimated by the non-regularized QR, where only variables that belongs to $S_\lambda$ are input:
\begin{IEEEeqnarray*}{lr} (\mathcal{L}_{\lambda}^{*},\beta_{\lambda}^{*})\overset{(obj,var)}{\longleftarrow} \underset{\beta_{0j},\beta_j,z_{p j}, \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{min}} \sum_{j \in J}  \sum_{t\in T}\left(\alpha_j \varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{tj}^{-}\right), \span \nonumber
\\
\text{subject to: } \span \nonumber \\
\varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}=y_{t} - \beta_{0\alpha} - \sum_{p\in S_\lambda} \beta_p x_{t,p}, & \forall t \in T, \forall j \in J, \\
\span \forall j \in J_{(-1)}, \forall p\in P, \\
\beta_{0j} + \beta_{j}^T x_{t} \leq \beta_{0,j+1} + \beta_{j+1}^T x_{t}, & \forall t \in T, \forall j \in J_{(-1)},\\
\varepsilon_{tj}^+,\varepsilon_{tj}^- \geq 0, &  \forall t \in T,\forall j \in J,\\ 
	 D2_{pj}^+, D2_{pj}^- \geq 0, & \forall j \in J,  \forall p\in P.
\end{IEEEeqnarray*}
The variable $\mathcal{L}_{\lambda}^{*}$ receives the value of the objective function on its optimal solution.
In summary, the optimization in equation \ref{eq:l1-qar-optim} acts as a variable selection for the subsequent estimation, which is normally called the post-LASSO estimation \cite{belloni2009least}.

\subsection{Regularization on the quantiles} \label{sec:regularization-quantiles}

\todoi{(talvez) Mostrar previsão com LASSO e seleção de lags.}


